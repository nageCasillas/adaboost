{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49eca738-2010-4f05-9c2f-dce9195a414e",
   "metadata": {},
   "source": [
    "# Mathematical Intuition Behind AdaBoost Boosting Technique\n",
    "## Introduction to AdaBoost  \n",
    "AdaBoost (Adaptive Boosting) is a powerful ensemble technique that combines multiple \"weak learners\" to form a strong predictive model. Each weak learner attempts to correct the errors made by previous learners by focusing more on the misclassified instances. This is achieved by dynamically adjusting the weights of data points after each iteration.\n",
    "\n",
    "## Weak Learners  \n",
    "A weak learner is a model that performs slightly better than random guessing (accuracy > 50% for binary classification). Decision stumps (single-level decision trees) are commonly used as weak learners in AdaBoost.\n",
    "\n",
    "## AdaBoost Model Construction  \n",
    "AdaBoost iteratively trains weak classifiers $f_m(x)$ and assigns them weights based on their performance. The final strong classifier is computed as a weighted sum of these weak learners:\n",
    "\n",
    "$F(x) = \\sum_{m=1}^{M} \\alpha_m f_m(x)$\n",
    "\n",
    "Where:\n",
    "- $\\alpha_m$ is the weight assigned to weak learner $f_m(x)$\n",
    "- $f_m(x)$ is the output of the weak learner\n",
    "\n",
    "### Weight Update  \n",
    "After each weak learner, the weights of incorrectly classified samples are increased, making them more important for the next learner.\n",
    "The weight update for sample $i$ is given by:\n",
    "\n",
    "$w_i^{(m+1)} = w_i^{(m)} \\times e^{\\alpha_m \\cdot I(y_i \\neq f_m(x_i))}$\n",
    "\n",
    "Where:\n",
    "- $w_i^{(m)}$ is the current weight of sample $i$\n",
    "- $\\alpha_m$ is the weight of the current weak learner\n",
    "- $I(y_i \\neq f_m(x_i))$ is an indicator function that equals 1 if the sample is misclassified and 0 otherwise\n",
    "\n",
    "## Example Dataset  \n",
    "| Feature 1 ($X_1$) | Feature 2 ($X_2$) | Label ($Y$) |\n",
    "|----------------|----------------|-----------|\n",
    "| 1              | 2              | +1        |\n",
    "| 2              | 1              | -1        |\n",
    "| 1              | 1              | +1        |\n",
    "| 2              | 2              | -1        |\n",
    "\n",
    "## Decision Stump  \n",
    "A decision stump is a weak learner that splits the data based on a single feature. For example:\n",
    "- **Stump 1:** If $X_1 < 1.5$, predict +1; otherwise, predict -1\n",
    "- **Stump 2:** If $X_2 < 1.5$, predict +1; otherwise, predict -1\n",
    "\n",
    "## Choosing the Best Stump (Feature Selection) Using Entropy  \n",
    "To select the best feature for the stump, we compute the **information gain** by evaluating the entropy before and after splitting:\n",
    "\n",
    "### Entropy Formula\n",
    "$H(S) = - \\sum_{c \\in \\{+1, -1\\}} p(c) \\log_2 p(c)$\n",
    "\n",
    "Where $p(c)$ is the proportion of class $c$ in the dataset.\n",
    "\n",
    "## Weight Normalization  \n",
    "The weights $w_i$ are normalized after each iteration to ensure they sum to 1:\n",
    "\n",
    "$w_i^{(m+1)} \\leftarrow \\frac{w_i^{(m+1)}}{\\sum_{i=1}^{N} w_i^{(m+1)}}$\n",
    "\n",
    "## Giving Weights to the Second Weak Learner  \n",
    "1. Compute the error rate of the first weak learner:\n",
    "   \n",
    "   $\\epsilon_m = \\frac{\\sum_{i=1}^{N} w_i^{(m)} I(y_i \\neq f_m(x_i))}{\\sum_{i=1}^{N} w_i^{(m)}}$\n",
    "\n",
    "2. Compute the weight for the weak learner:\n",
    "   \n",
    "   $\\alpha_m = \\frac{1}{2} \\log \\left( \\frac{1 - \\epsilon_m}{\\epsilon_m} \\right)$\n",
    "\n",
    "3. Update the weights for the next weak learner.\n",
    "\n",
    "By iteratively adjusting weights and focusing on difficult samples, AdaBoost builds a strong ensemble model capable of achieving high accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
